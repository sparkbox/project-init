{
  "name": "natural",
  "description": "General natural language (tokenizing, stemming, classification, inflection, phonetics, tfidf, WordNet) facilities for node.",
  "version": "0.0.65",
  "homepage": "https://github.com/NaturalNode/natural",
  "engines": {
    "node": ">=0.4.9"
  },
  "dependencies": {
    "sylvester": ">= 0.0.12",
    "apparatus": ">= 0.0.4",
    "underscore": "*"
  },
  "devDependencies": {
    "uubench": "0.0.x"
  },
  "author": {
    "name": "Chris Umbel",
    "email": "chris@chrisumbel.com"
  },
  "keywords": [
    "natural",
    "language",
    "porter",
    "lancaster",
    "stemmer",
    "bayes",
    "classifier",
    "phonetic",
    "metaphone",
    "inflector",
    "wordnet",
    "tf-idf",
    "logistic",
    "regression"
  ],
  "main": "./lib/natural/index.js",
  "maintainers": [
    {
      "name": "Chris Umbel",
      "email": "chris@chrisumbel.com",
      "url": "http://www.chrisumbel.com"
    },
    {
      "name": "Rob Ellis",
      "email": "rob@silentrob.me"
    }
  ],
  "readme": "\nnatural\n=======\n\n\"Natural\" is a general natural language facility for nodejs. Tokenizing,\nstemming, classification, phonetics, tf-idf, WordNet, and some inflection are \ncurrently supported.\n\nIt's still in the early stages, and am very interested in bug reports,\ncontributions and the like.\n\nNote that many algorithms from Rob Ellis's [node-nltools](https://github.com/NaturalNode/node-nltools) are\nbeing merged in to this project and will be maintained here going forward.\n\nAt the moment most algorithms are English-specific but long-term some diversity\nis in order.\n\nAside from this README the only current documentation is [here on my blog](http://www.chrisumbel.com/article/node_js_natural_language_porter_stemmer_lancaster_bayes_naive_metaphone_soundex).\n\nInstallation\n------------\n\nIf you're just looking to consume natural without your own node application\nplease install the NPM\n\n    npm install natural\n    \nIf you're interested in contributing to natural or just hacking it then by all\nmeans fork away!\n    \nTokenizers\n----------\n\nWord, Regexp and Treebank tokenizers are provided for breaking text up into\narrays of tokens.\n\n    var natural = require('natural'),\n      tokenizer = new natural.WordTokenizer();\n    console.log(tokenizer.tokenize(\"your dog has flees.\"));\n    // [ 'your', 'dog', 'has', 'flees' ]\n    \nThe other tokenizers follow a similar pattern    \n\n    tokenizer = new natural.TreebankWordTokenizer();\n    console.log(tokenizer.tokenize(\"my dog hasn't any flees.\"));\n    // [ 'my', 'dog', 'has', 'n\\'t', 'any', 'flees', '.' ]\n    \n    tokenizer = new natural.RegexpTokenizer({pattern: /\\-/});\n    console.log(tokenizer.tokenize(\"flee-dog\"));\n    // [ 'flee', 'dog' ]\n    \nStemmers\n--------\n\nCurrently stemming is supported via the Porter and Lancaster (Paice/Husk)\nalgorithms.\n\n    var natural = require('natural');\n    \nthis example uses a porter stemmer. \"word\" is returned.\n\n    console.log(natural.PorterStemmer.stem(\"words\")); // stem a single word\n    \nattach() patches stem() and tokenizeAndStem() to String as a shortcut to\nPorterStemmer.stem(token). tokenizeAndStem() breaks text up into single words\nand returns an array of stemmed tokens.\n\n    natural.PorterStemmer.attach();\n    console.log(\"i am waking up to the sounds of chainsaws\".tokenizeAndStem());\n    console.log(\"chainsaws\".stem());\n\nthe same thing can be done with a lancaster stemmer\n\n    natural.LancasterStemmer.attach();\n    console.log(\"i am waking up to the sounds of chainsaws\".tokenizeAndStem());\n    console.log(\"chainsaws\".stem());\n\nClassifiers\n----------------------\n\nTwo classifiers are currently supported, Naive Bayes and logistic regression.\nThe following examples use the BayesClassifier class, but the \nLogisticRegressionClassifier class could be substituted instead.\n\n    var natural = require('natural'), \n    \tclassifier = new natural.BayesClassifier();\n\nyou can train the classifier on sample text. it will use reasonable defaults to\ntokenize and stem the text.\n   \n    classifier.addDocument('i am long qqqq', 'buy');\n    classifier.addDocument('buy the q's', 'buy');\n    classifier.addDocument('short gold', 'sell');\n    classifier.addDocument('sell gold', 'sell');\n\n    classifier.train();\n\noutputs \"sell\"\n\n    console.log(classifier.classify('i am short silver'));\n\noutputs \"buy\"\n\n    console.log(classifier.classify('i am long copper'));\n\n    classifier = new natural.BayesClassifier();\n\nthe classifier can also be trained on and classify arrays of tokens, strings, or\nany mixture. arrays let you use entirely custom data with  your own\ntokenization/stemming if any at all.\n\n    classifier.addDocument(['sell', 'gold'], 'sell');\n\nA classifier can also be persisted and recalled so you can reuse a training\n\n    classifier.save('classifier.json', function(err, classifier) {\n        // the classifier is saved to the classifier.json file!\n    });\n    \nand to recall from the classifier.json saved above:\n\n    natural.BayesClassifier.load('classifier.json', null, function(err, classifier) {\n        console.log(classifier.classify('long SUNW'));\n        console.log(classifier.classify('short SUNW'));\n    });\n\nA classifier can also be serialized and deserialized as such\n\n    var classifier = new natural.BayesClassifier();\n    classifier.addDocument(['sell', 'gold'], 'sell');\n    classifier.addDocument(['buy', 'silver'], 'buy');\n\n    // serialize\n    var raw = JSON.stringify(classifier);\n    // deserialize\n    var restoredClassifier = natural.BayesClassifier.restore(raw);\n    console.log(restoredClassifier.classify('i should sell that'));\n\nPhonetics\n---------\n\nPhonetic matching (sounds-like) matching can be done with either the SoundEx or\nMetaphone algorithms\n\n    var natural = require('natural'),\n        metaphone = natural.Metaphone, soundEx = natural.SoundEx;\n\n    var wordA = 'phonetics';\n    var wordB = 'fonetix';\n\ntest the two words to see if they sound alike\n\n    if(metaphone.compare(wordA, wordB))\n        console.log('they sound alike!');\n        \nthe raw phonetics are obtained with process()\n\n    console.log(metaphone.process('phonetics'));\n\nattaching will patch String with useful methods\n\n    metaphone.attach();\n\nsoundsLike is essentially a shortcut to Metaphone.compare\n\n    if(wordA.soundsLike(wordB))\n        console.log('they sound alike!');\n        \nthe raw phonetics are obtained with phonetics()\n\n    console.log('phonetics'.phonetics());   \n\nfull text strings can be tokenized into arrays of phonetics similar to stemmers\n\n    console.log('phonetics rock'.tokenizeAndPhoneticize());\n\nsame module operations apply with SoundEx\n\n    if(soundEx.compare(wordA, wordB))\n        console.log('they sound alike!');\n\n    console.log(soundEx.process('phonetics'));\n\nthe same String patches apply with soundex\n\n    soundEx.attach();\n\n    if(wordA.soundsLike(wordB))\n        console.log('they sound alike!');\n        \n    console.log('phonetics'.phonetics());\n    \n    \nInflectors\n----------\n\n### Nouns\n\nNouns can be pluralized/singularized with a NounInflector \n\n    var natural = require('natural'),\n    nounInflector = new natural.NounInflector();\n    \nto pluralize a word (outputs \"radii\")\n\n    console.log(nounInflector.pluralize('radius'));\n\nto singularize a word (outputs \"beer\")\n\n    console.log(nounInflector.singularize('beers'));\n\nLike many of the other features String can be patched to perform the operations\ndirectly. The \"Noun\" suffix to the methods is necessary as verbs will be\nsupported in the future.\n\n    nounInflector.attach();\n    console.log('radius'.pluralizeNoun());\n    console.log('beers'.singularizeNoun());   \n\n### Numbers\n\nNumbers can be counted with a CountInflector\n\n    var countInflector = natural.CountInflector;\n\noutputs \"1st\"\n\n    console.log(countInflector.nth(1));\n\noutputs \"111th\"\n\n    console.log(countInflector.nth(111));\n\n### Present Tense Verbs\n\nPresent Tense Verbs can be pluralized/singularized with a PresentVerbInflector.\nThis feature is still experimental as of 0.0.42 so use with caution and please\nprovide feedback.\n\n    var verbInflector = new natural.PresentVerbInflector();\n\noutputs \"becomes\"\n\n    console.log(verbInflector.singularize('become'));\n\noutputs \"become\"\n\n    console.log(verbInflector.pluralize('becomes'));\n\nLike many other natural modules attach() can be used to patch strings with\nhandy methods.\n\n    verbInflector.attach();\n    console.log('walk'.singularizePresentVerb());\n    console.log('walks'.pluralizePresentVerb());\n\n\nN-Grams\n-------\n\nn-grams can be obtained for either arrays or strings (which will be tokenized\nfor you)\n\n    var NGrams = natural.NGrams;\n    \n### bigrams    \n    \n    console.log(NGrams.bigrams('some words here'));\n    console.log(NGrams.bigrams(['some',  'words',  'here']));\n    \nboth of which output [ [ 'some', 'words' ], [ 'words', 'here' ] ]    \n    \n### trigrams    \n    \n    console.log(NGrams.trigrams('some other words here'));\n    console.log(NGrams.trigrams(['some',  'other', 'words',  'here']));\n\nboth of which output [ [ 'some', 'other', 'words' ],\n  [ 'other', 'words', 'here' ] ]\n\n### arbitrary n-grams\n\n    console.log(NGrams.ngrams('some other words here for you', 4));\n    console.log(NGrams.ngrams(['some', 'other', 'words', 'here', 'for',\n        'you'], 4));\n\nwhich outputs [ [ 'some', 'other', 'words', 'here' ],\n  [ 'other', 'words', 'here', 'for' ],\n  [ 'words', 'here', 'for', 'you' ] ]\n\ntf-idf\n-----\n\nTerm Frequencyâ€“Inverse Document Frequency (tf-idf) is implemented to determine how important a word (or words) is to a \ndocument relative to a corpus. The following example will add four documents to \na corpus and determine the weight of the word \"node\" and then the weight of the \nword \"ruby\" in each document.\n\n    var natural = require('natural'),\n        TfIdf = natural.TfIdf,\n        tfidf = new TfIdf();\n    \n    tfidf.addDocument('this document is about node.');\n    tfidf.addDocument('this document is about ruby.');\n    tfidf.addDocument('this document is about ruby and node.');\n    tfidf.addDocument('this document is about node. it has node examples');\n    \n    console.log('node --------------------------------');\n    tfidf.tfidfs('node', function(i, measure) {\n        console.log('document #' + i + ' is ' + measure);\n    });\n\n    console.log('ruby --------------------------------');\n    tfidf.tfidfs('ruby', function(i, measure) {\n        console.log('document #' + i + ' is ' + measure);\n    });\n\nwhich outputs\n    \n    node --------------------------------\n    document #0 is 1.4469189829363254\n    document #1 is 0\n    document #2 is 1.4469189829363254\n    document #3 is 2.8938379658726507\n    ruby --------------------------------\n    document #0 is 0\n    document #1 is 1.466337068793427\n    document #2 is 1.466337068793427\n    document #3 is 0\n\nOf course you can measure a single document. The following example measures \nthe term \"node\" in the first and second documents.\n    \n    console.log(tfidf.tfidf('node', 0));\n    console.log(tfidf.tfidf('node', 1));\n\nA TfIdf instance can also load documents from files on disk.\n\n    var tfidf = new TfIdf();\n    tfidf.addFileSync('data_files/one.txt');\n    tfidf.addFileSync('data_files/two.txt');\n\nMultiple terms can be measured as well with their weights being added into \na single measure value. The following example determines that the last document\nis the most relevent to the words \"node\" and \"ruby\".\n\n    var natural = require('natural'),\n        TfIdf = natural.TfIdf,\n        tfidf = new TfIdf();\n    \n    tfidf.addDocument('this document is about node.');\n    tfidf.addDocument('this document is about ruby.');\n    tfidf.addDocument('this document is about ruby and node.');\n    \n    tfidf.tfidfs('node ruby', function(i, measure) {\n        console.log('document #' + i + ' is ' + measure);\n    });\n\nwhich outputs\n\n    document #0 is 1.2039728043259361\n    document #1 is 1.2039728043259361\n    document #2 is 2.4079456086518722\n\nThe examples above all use strings in which case natural will tokenize the input.\nIf you wish to perform your own tokenization or other kinds of processing you \ncan do so and then pass in the resultant arrays. That will cause natural to \nbypass its own preprocessing.\n\n    var natural = require('natural'),\n        TfIdf = natural.TfIdf,\n        tfidf = new TfIdf();\n    \n    tfidf.addDocument(['document', 'about', 'node']);\n    tfidf.addDocument(['document', 'about', 'ruby']);\n    tfidf.addDocument(['document', 'about', 'ruby', 'node']);\n    tfidf.addDocument(['document', 'about', 'node', 'node', 'examples']);\n    \n    tfidf.tfidfs(['node', 'ruby'], function(i, measure) {\n        console.log('document #' + i + ' is ' + measure);\n    });\n\nIt's possible to retrieve a list of all terms in a document sorted by their \nimportance.\n\n    tfidf.listTerms(0 /*document index*/).forEach(function(item) {\n        console.log(item.term + ': ' + item.tfidf);\n    });\n\nA TfIdf instance can also be serialized and deserialzed for save and recall.\n\n    var tfidf = new TfIdf();\n    tfidf.addDocument('document one', 'un');\n    tfidf.addDocument('document Two', 'deux');\n    var s = JSON.stringify(tfidf);\n    // save \"s\" to disk, database or otherwise\n\n    // assuming you pulled \"s\" back out of storage. \n    var tfidf = new TfIdf(JSON.parse(s));\n\nWordNet\n-------\n\nOne of the newest and most experimental features is WordNet integration. Here's an\nexample of using natural to look up definitions of the word node. The parameter in\nthe WordNet constructor is the local directory that will store the WordNet \ndatabase files. If the database files are not present in the specified directories\nnatural will download them for you.\n\nKeep in mind the WordNet integration is to be considered experimental at this point\nand not production ready. The API is also subject to change.\n\nHere's an exmple of looking up definitions for the word, \"node\".\n\n    var wordnet = new natural.WordNet('.');\n\n    wordnet.lookup('node', function(results) {\n        results.forEach(function(result) {\n            console.log('------------------------------------');\n            console.log(result.synsetOffset);\n            console.log(result.pos);\n            console.log(result.lemma);\n            console.log(result.synonyms);\n            console.log(result.pos);\n            console.log(result.gloss);\n        });\n    });\n    \nGiven a synset offset and part of speech a definition can be looked up directly.\n\n    var wordnet = new natural.WordNet('.');\n\n    wordnet.get(4424418, 'n', function(result) {\n        console.log('------------------------------------');\n        console.log(result.lemma);\n        console.log(result.pos);\n        console.log(result.gloss);\n        console.log(result.synonyms);        \n    });\n\nPrinceton University \"About WordNet.\" WordNet. Princeton University. 2010. <http://wordnet.princeton.edu>\n\nLicense\n-------\n\nCopyright (c) 2011, Chris Umbel, Rob Ellis\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nWordNet License\n---------------\n\nThis license is available as the file LICENSE in any downloaded version of WordNet. \nWordNet 3.0 license: (Download)\n\nWordNet Release 3.0 This software and database is being provided to you, the LICENSEE, by Princeton University under the following license. By obtaining, using and/or copying this software and database, you agree that you have read, understood, and will comply with these terms and conditions.: Permission to use, copy, modify and distribute this software and database and its documentation for any purpose and without fee or royalty is hereby granted, provided that you agree to comply with the following copyright notice and statements, including the disclaimer, and that the same appear on ALL copies of the software, database and documentation, including modifications that you make for internal use or for distribution. WordNet 3.0 Copyright 2006 by Princeton University. All rights reserved. THIS SOFTWARE AND DATABASE IS PROVIDED \"AS IS\" AND PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANT- ABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS. The name of Princeton University or Princeton may not be used in advertising or publicity pertaining to distribution of the software and/or database. Title to copyright in this software, database and any associated documentation shall at all times remain with Princeton University and LICENSEE agrees to preserve same.",
  "readmeFilename": "README.md",
  "_id": "natural@0.0.65",
  "dist": {
    "shasum": "cab2e544b2240bb2b4bffdaa9e02abc94a1bd460"
  },
  "_from": "natural@0.0.65"
}
